{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmAK4jFCjyH3"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__pVKLSsj-iP"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "import os\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_TOKEN\")\n",
    "login(token=\"hf_api_key\")\n",
    "\n",
    "ds = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRpnLPEby1sC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "embeding_dim = 768\n",
    "vocab_size = 30522\n",
    "n_layer = 12\n",
    "block_size = 512\n",
    "\n",
    "\n",
    "class myBertForMaskedLM(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, block_size, n_layer):\n",
    "      super().__init__()\n",
    "      self.bert = BertModel(vocab_size, embedding_dim, block_size, n_layer)\n",
    "      self.pooling = BertPooling()\n",
    "\n",
    "  def forward(self, x, attention_mask):\n",
    "    x = self.bert(x, attention_mask)\n",
    "    x = self.pooling(x, attention_mask)\n",
    "    return x\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, block_size, n_layer):\n",
    "      super().__init__()\n",
    "      self.embeddings = BertEmbeddings(vocab_size, embedding_dim, block_size)\n",
    "      self.encoder = BertEncoder(n_layer,embeding_dim)\n",
    "\n",
    "  def forward(self, x, attention_mask):\n",
    "      x = self.embeddings(x)\n",
    "      x = self.encoder(x, attention_mask)\n",
    "      return x\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, block_size, type_vocab_size=2):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embeddings = nn.Embedding(block_size, embedding_dim)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, embedding_dim)\n",
    "        self.LayerNorm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        B, T = input_ids.size()\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros((B, T), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        word_emb = self.word_embeddings(input_ids)\n",
    "        pos_ids = torch.arange(T, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "        pos_emb = self.position_embeddings(pos_ids)\n",
    "        token_type_emb = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "\n",
    "        emb = word_emb + pos_emb\n",
    "        emb = self.LayerNorm(emb)\n",
    "        return self.dropout(emb)\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, n_layer, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(n_embed) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        for layer in self.layer:\n",
    "            x = layer(x, attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(n_embed)\n",
    "        self.intermediate = BertIntermediate(n_embed)\n",
    "        self.output = BertOutput(n_embed)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        x = self.attention(x, attention_mask)\n",
    "        x = self.intermediate(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.self = BertSdpaSelfAttention(n_embed)\n",
    "        self.output = BertSelfOutput(n_embed)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        x = self.self(x, attention_mask)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertSdpaSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embed, n_embed)\n",
    "        self.key = nn.Linear(n_embed, n_embed)\n",
    "        self.value = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        att = q @ k.transpose(-2, -1) / math.sqrt(self.n_embed)\n",
    "\n",
    "        att = att.masked_fill(attention_mask.unsqueeze(1) == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = att @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(n_embed, n_embed)\n",
    "        self.LayerNorm = nn.LayerNorm(n_embed)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense(x)\n",
    "        out = self.dropout(self.LayerNorm(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(n_embed, 4 * n_embed)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.intermediate_act_fn(self.dense(x))\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(4 * n_embed, n_embed)\n",
    "        self.LayerNorm = nn.LayerNorm(n_embed)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(self.LayerNorm(x))\n",
    "        return x\n",
    "\n",
    "#The crux of this notebooks !!! :)\n",
    "class BertPooling(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "  def forward(self, x, attention_mask):\n",
    "    # x => B,T, C\n",
    "    mask = attention_mask.unsqueeze(-1) #B,T,1\n",
    "    x = x * mask  #B,T,C\n",
    "    lengths = mask.sum(dim=1).clamp(min=1)\n",
    "    x_mean = x.sum(dim=1) / lengths\n",
    "\n",
    "    return x_mean\n",
    "\n",
    "\n",
    "#We steal the weights of pretrained BERT, so we don't need to traine the model :)\n",
    "\n",
    "mymodel = myBertForMaskedLM(vocab_size=vocab_size, embedding_dim=embeding_dim, block_size=block_size, n_layer=n_layer)\n",
    "mymodel.load_state_dict(model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkMsseHGY7fd"
   },
   "outputs": [],
   "source": [
    "#Just a check to make sur I have the same weight than pretrained BERT\n",
    "\n",
    "for (name1, param1), (name2, param2) in zip(model.named_parameters(), mymodel.named_parameters()):\n",
    "    if name1 == name2:\n",
    "        same = torch.allclose(param1, param2, atol=1e-6)\n",
    "        print(f\"{name1}: {'OK' if same else 'DIFF'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWvUOUu9QEXv"
   },
   "outputs": [],
   "source": [
    "#I create my Dataloader, to fine tune it , on this dataset we will focus on sentence1, sentence2, and score. So I load this 3 features within batches, batch_st1, batch_st2, y\n",
    "\n",
    "class Dataloader:\n",
    "  def __init__(self,B,T):\n",
    "    self.B = B\n",
    "    self.T = T\n",
    "\n",
    "  def _generate_batch(self, idx , mode='train'):\n",
    "\n",
    "\n",
    "    batch_1 = tokenizer(\n",
    "    ds[\"train\"][idx: self.B + idx + 1][\"sentence1\"],\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "    batch_2 = tokenizer(\n",
    "    ds[\"train\"][i: self.B + idx + 1][\"sentence2\"],\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "    y = torch.tensor(ds[\"train\"][idx: self.B + idx + 1][\"score\"]) / 5.0\n",
    "\n",
    "    return batch_1, batch_2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znEjTg9yXzLJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "sentences = [\"The cat slept peacefully while the car crashed loudly into the wall\", \"Stock markets are experriencing a major downturn\"]\n",
    "inputs = tokenizer(sentences, padding='longest', return_tensors='pt')\n",
    "res = mymodel(inputs.inputs_ids, inputs.attention_mask)\n",
    "\n",
    "#On recup l\"embedding des 2 tokens, pour\n",
    "\n",
    "token_2 = tokenizer.convert_tokens_to_ids(\"peacefully\")\n",
    "token_1 = tokenizer.convert_tokens_to_ids(\"crashed\")\n",
    "B,T, c = res.shape\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    if batch[b,t].item() == token_1:\n",
    "      coor_1 = b,t\n",
    "    elif batch[b,t].item() == token_2:\n",
    "      coor_2 = b,t\n",
    "embedding_token_1 = res[coor_1[0],coor_1[1] , :]\n",
    "embedding_token_2 =  res[coor_2[0],coor_2[1] , :]\n",
    "\n",
    "\n",
    "\n",
    "F.cosine_similarity(embedding_token_1,embedding_token_2, dim=0)\n",
    "\n",
    "#NOTE: I'm quite surprised to see that semantically similar tokens are not actually close in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "id": "9mSyQhyMVMRU",
    "outputId": "1af418b8-8516-470b-aaf2-2579d48b259f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d = Dataloader(B=16, T=32)\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-4)\n",
    "loss_obj = nn.MSELoss()\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=150,\n",
    "    eta_min=1e-4\n",
    ")\n",
    "\n",
    "#Let's train on 10 epochs\n",
    "\n",
    "for epoch in range(10):\n",
    "    idxs = torch.randint(0, len(ds['train']) - d.B, (32,))  # 32 batchs par epoch\n",
    "    for i in idxs:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_1, batch_2, y = d._generate_batch(idx=i.item())\n",
    "\n",
    "        emb_1 = mymodel(batch_1.input_ids, batch_1.attention_mask)\n",
    "        emb_2 = mymodel(batch_2.input_ids, batch_2.attention_mask)\n",
    "\n",
    "        sim = F.cosine_similarity(emb_1, emb_2, dim=-1)\n",
    "        loss = loss_obj(sim, y)\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rryQjFyV93C"
   },
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bckxMp_XOC_D"
   },
   "outputs": [],
   "source": [
    "#Exemple of sentence\n",
    "sentence1 = \"A cat is sleeping on the couch.\"\n",
    "sentence2 = \"A dog is resting on the sofa.\"\n",
    "\n",
    "# Tokenization\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=32)\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=32)\n",
    "\n",
    "# Calcul des embeddings\n",
    "with torch.no_grad():\n",
    "    emb1 = mymodel(inputs1[\"input_ids\"], inputs1[\"attention_mask\"])  # [1, C]\n",
    "    emb2 = mymodel(inputs2[\"input_ids\"], inputs2[\"attention_mask\"])  # [1, C]\n",
    "\n",
    "# Similarit√© cosinus\n",
    "sim = F.cosine_similarity(emb1, emb2, dim=-1)  # [1]\n",
    "print(f\"Cosine similarity: {sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atcd29krh3NW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ctqlQRESxMR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
