{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aiYhHpVJHv1",
    "outputId": "59d490cf-0f72-4fff-cd7e-d49d08b6e320"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq -U transformers datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "69cc84788e5f4cda878a6735e91d1540",
      "39fc3c8b41124ec289b5c055112afabc",
      "9076254e51a14e0581a8a3f07773afe0",
      "4fbc21a2ca864dbb859369e2727f11d6",
      "122152e2dee340b488a4d0643463e7b5",
      "1d7415ebeaf0484c9582cfdb7115470b",
      "77e8fbf184554c9dacc1459df9c80ddb",
      "e529a8ef8c1f40cd9fec8526dbc86496",
      "8599da5a5807440ba68b330d094e4c16",
      "47f925b6000d460695f798522a570685",
      "211373e5510e49a68074feb9b8807383",
      "52cd6dee10454c6ea7bd7f13b37310fa",
      "48a98f9475a1446cb2077c5d9b20689b",
      "ecc3176d5faf4e4da6d048fafdc10628",
      "c8f52e02f727431c9394aae5cbe67037",
      "a9dae4e986d14cd1889d8bc75fffb6ce",
      "d60b2d74e738472bb7cfce46e40b3bf1",
      "6de73d9243ea44ca8f5ad63926f42ed0",
      "25127dcff9a14c29850e89859c5a65d5",
      "c4d81fd12e7c4d09bbf80e3ae70587cf",
      "b7cfc96dad5b4d5d93c11b73d2c255eb",
      "e3b4c8f85b084ad9af1d2826e697e1a1",
      "59dfcfadfc424ec8ac3314f567789205",
      "73e0bcb80bab4e4d9b796d5f9150f70a",
      "0264de530ef44f0a8bad31c43ed96669",
      "6e3d7bc7776d49d5919c327463726c96",
      "f3a7b6087de949fdbd96ea137bd153ab",
      "e7b0f3e3ac304854ae51eb39a460a1a4",
      "768b0693fb29458d93bb6e5cdba7fbbe",
      "2608e57cae3640e0bf08973758fd6fcb",
      "4c983a37a7834a7c944bcad5237697a7",
      "5ecbbe51da4c46dd80a07f269ec28790",
      "4a233ae862604f488f9832b5273f1c4f",
      "636b20d9dceb4143ae8bf5b95da064cd",
      "1e2600be0e0b4a99b24726e3e2a08b8f",
      "302ddfd0003b4ce38bdbc98b3634fb4b",
      "b331cf3e573549a68c83ba7ece74dafa",
      "fd5ecf2db5da4c33a72f5d95d592455f",
      "35d200ef84434d819299ee4cf3e34603",
      "ddaa776bd43b4a42b4eb0cc9cff11f1d",
      "2334c14a95614dee9eff386aaa340384",
      "5189f97e61004bbdaef0da055fb4cbf4",
      "09a3b326d5a64662a1f40560b87a1181",
      "044a6aeab2654d299e54103c901050a9",
      "1b11981172bd4a18b40608c0138822d2",
      "1c8ddb5fa2bd42a08f3ace5ab260150e",
      "147a0c86ab63462b8459dc60bc31aa8c",
      "e72729c4596046838e5c21ab67fb53c3",
      "f4274657d4e74ee4a4ebc5aad444c03f",
      "63390dd31084437abe855c7dabf660d4",
      "6d169cde0b1c42809f69ef4b54353954",
      "26e967299dd948ebbdedef8d2fbac208",
      "f8206e76fa3b41ddb3a0de20136cdb2e",
      "1eb9c527c80b4b5f875b2125dfd78dd2",
      "5a7712f935f44758856aa35b04675b09"
     ]
    },
    "id": "xoRGkj6i4zXp",
    "outputId": "f0e9c52b-c0d6-4b71-ea73-bc172c75242e"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_TOKEN\")\n",
    "login(token=\"hf_api_key\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPagY9Cf5Z56",
    "outputId": "cf9a25e5-d747-42f2-8344-36052db58bc4"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "a38240a4724745628a34cbc21ab4a8f4",
      "643c1183934d492c8c346c2a22930308",
      "c728485a0718482cb1a205e693c77715",
      "74074c2ed3ad4ebcad519eb7b7bd1062",
      "c36002dca3224d2fa5d01d98b3c50f5c",
      "efd59bc87b1040c087ef0a3cd9f1c36d",
      "c5e7dad5fb474d1e862d8c9c039a6a5d",
      "51174ff78381421aafc7f4c2ca96a3e5",
      "99102760da53498dbdbe6c0ef2b55e53",
      "f74952e8bc594ffc8374be26ceaa3818",
      "b5fed10a7c754bf8b1ec0164ec3c7f64",
      "dfeb7f23bfe84b6db207c2a18772b899",
      "d4d7b77e1308451da6aa15fb6c6fb974",
      "537ae995464341d5bff577f7618f20e0",
      "42d6ea90465942eab65d2d28b3eb0f0f",
      "eed4888d8b1e4176bc58e40321787187",
      "914430d3f3474f0ebbbf1faa61aceffe",
      "c26c8619c122469f9eca0edf4cc3cba6",
      "875b74e467784684af5dd3da7c67d94d",
      "7f60dafd74af407199f13c10665b65a9",
      "c881853ac4214f4b8da3a6672f6be2b2",
      "435976bae4ab4ec6916356f9c528e306",
      "2f1c6645fd2d42f0b091390445d41bbc",
      "d8f80a967da24ec49c78f24187d7ba5b",
      "8bfd701b06aa4c96a286c465f2e75c4d",
      "f17bdfa90ab14597b78672838da72ee1",
      "a4c1366e79c244f5b0204dc1e29d1141",
      "0eb94462053f4faabcda079383a0c31f",
      "a3256f1a963b4fcb931f9383d096fe3f",
      "e8347a4f64a84337bf253e0f18ce30f4",
      "a3dfb533b415458d820c2799f04231d3",
      "c0066f70b5fc4de79c5df1a69f7c576e",
      "592096c3bae64b64964daa7fc745ef3d",
      "9ea3256fc8a248619ac0b4da10521f56",
      "ced68a0672c24d70bd29383d82dabaca",
      "84e5e1e3f5ce48858f223c6bdfb21482",
      "4a15fc533ec14e748a53c3d59d10b73e",
      "041d4abb058643269c68c409c7efbbdc",
      "cb65b03199b44db4b1fae158f4d0af0f",
      "2491879b29b2494cba29a2dd464da930",
      "ea6a040104124248a3910b0e1f6132e6",
      "ac0df25aacbc40d281eef2288bea075c",
      "86439cfddf314743bc8dd37e64cdd302",
      "7165035efab64a14bdeb4924939a1ccc",
      "b026195a53d0449c854867d6aeb01a4d",
      "ec195ba5e8534d82a1de2913ed13e814",
      "9847e8f95c9e400b8de1001ab7e0a3b4",
      "9ac901c1f20b4ec2b136038f47f370b9",
      "497f8167bbc344e3b759f417deb947d6",
      "8efb99065c514c05b5b7f36c9d4f7775",
      "a967d5ad4a854c39853be8821cf870a0",
      "e20f5ef39edf468988c4ed5764e3263e",
      "3140498c37434175957d3d015d5cad32",
      "5221f41c1132409ab9ca359f4ee3b737",
      "979de31103b94fca89262d49a4f27171",
      "6772efdd5e9a4d0186b3cdd2db264b84",
      "11d6ef7c27b74a1fb7488e523e939f0b",
      "1647b2051dff4cc1a74c70e14106a22d",
      "0271841c43a94f37b1a5b0a87542c775",
      "5b75726024ef419ebca8459981d41a6b",
      "67d02685e6f14971ac3c7f64866cb44d",
      "42c3dfce8766466db12f66f053136219",
      "97a98e6167fa4f6e98608a2a89f73fa1",
      "b9039bbd2937468c8ea758c58ab3a535",
      "639c68beafb64f3d800ded43455e3309",
      "201d49f36f1f478494580c4d61ddcf31",
      "3f2f7c69dd5a4b7790ae2b20918f3030",
      "7bb54d49b1e44d18a3a5b4788f138b0d",
      "faf0cefb357240698ad8775d52837497",
      "eb33478f6be741b7a8bf93beebe16bbd",
      "f1bdc7322c93476ea897f6dbd62d6a6e",
      "e09c4df50c4548d686c1e4e0435d47c4",
      "836d0120da1941908d795e259e607a10",
      "f98309e4ffc84223a6a9136b3da122e3",
      "957414be35454a73905045e9bc0fab62",
      "0c750acb5c314a80adcfc73f81db4706",
      "bafa8880353c402a8c7c830023973ed3"
     ]
    },
    "id": "XXbEbhY05HRe",
    "outputId": "43a07a33-3473-4b61-ba37-8d92d8d3a2d3"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def formatting(example):\n",
    "    return {\n",
    "        \"prompt\": f\"### Context:\\n{example['context']}\\n\\n### question:\\n{example['question']}\",\n",
    "        \"answers\": f\"### answers:\\n{example['answers']['text'][0]}{tokenizer.eos_token}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3lS8SHWdZ46",
    "outputId": "6bcf076a-f1bf-4f04-b064-aa211476b8fd"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnhKn2OoZVQC"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T) -> None:\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "    def _processing(self, seq):\n",
    "        \"\"\"\n",
    "        Processes a token sequence to make sure it has exactly T+1 tokens,\n",
    "        so that we can safely do x = seq[:T] and y = seq[1:T+1] without any error.\n",
    "        \"\"\"\n",
    "        eos_tensor = torch.tensor(\n",
    "            tokenizer.encode(tokenizer.eos_token), dtype=seq.dtype\n",
    "        )\n",
    "\n",
    "        # Truncation (We keep last T tokens, and we add EOS tokens to reach T+1)\n",
    "        if len(seq) > self.T:\n",
    "            return torch.cat((seq[-self.T :], eos_tensor))\n",
    "\n",
    "        # Padding (We add EOS + padding EOS to reach T+1 tokens)\n",
    "        if len(seq) < self.T:\n",
    "            pad_len = self.T - len(seq) + 1\n",
    "            padding = torch.full((pad_len,), eos_tensor[0], dtype=seq.dtype)\n",
    "            return torch.cat((seq, padding))\n",
    "\n",
    "        # Exactly T â†’ we add EOS to reach T+1\n",
    "        return torch.cat((seq, eos_tensor))\n",
    "\n",
    "    def _processing_mask(self, seq):\n",
    "        \"\"\"\n",
    "        same logic here but with adding 0 for padding\n",
    "        \"\"\"\n",
    "\n",
    "        if len(seq) > self.T:\n",
    "            return torch.cat((seq[-self.T :], torch.tensor([0])))\n",
    "\n",
    "        if len(seq) < self.T:\n",
    "            pad_len = self.T - len(seq) + 1\n",
    "            padding = torch.full((pad_len,), 0, dtype=seq.dtype)\n",
    "            return torch.cat((seq, padding))\n",
    "\n",
    "        return torch.cat((seq, torch.tensor([0])))\n",
    "\n",
    "    def _generate_mask(self, input_ids, target_ids):\n",
    "        \"\"\"\n",
    "        Generates a mask by separating input and target tokens.\n",
    "\n",
    "        I chose to split input_ids and target_ids to create two distinct masks:\n",
    "        - 0 for input tokens (no backprop)\n",
    "        - 1 for target tokens (enable backprop)\n",
    "\n",
    "        The two masks are then concatenated. This final mask tells the model\n",
    "        which tokens should contribute to the loss and gradients.\n",
    "        \"\"\"\n",
    "\n",
    "        mask_input_ids = torch.zeros(input_ids.shape[0], dtype=torch.long)\n",
    "        mask_target_ids = torch.ones(target_ids.shape[0], dtype=torch.long)\n",
    "\n",
    "        return torch.cat((mask_input_ids, mask_target_ids))\n",
    "\n",
    "    def load_batch(self, dataset, mode):\n",
    "        \"\"\"\n",
    "        a function that create 3 batchs to feed to the model\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = dataset[\"train\"] if mode == \"train\" else dataset[\"test\"]\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        batch_mask = []\n",
    "\n",
    "        # We generate B index to pick B exemple from the dataset\n",
    "        idx = torch.randint(0, len(dataset), (self.B,))\n",
    "\n",
    "        # For each index, we create sequences x, y , mask\n",
    "        for id in idx:\n",
    "            input_ids = torch.tensor(\n",
    "                tokenizer.encode(dataset[\"prompt\"][id])\n",
    "            )  # 1D Tensor\n",
    "            target_ids = torch.tensor(\n",
    "                tokenizer.encode(dataset[\"answers\"][id])\n",
    "            )  # 1D Tensor\n",
    "            mask_ids = self._generate_mask(input_ids, target_ids)\n",
    "            seq_ids = torch.cat((input_ids, target_ids))\n",
    "\n",
    "            seq_ids = self._processing(seq_ids)\n",
    "            mask_ids = self._processing_mask(mask_ids)\n",
    "\n",
    "            x = seq_ids[: self.T]\n",
    "            y = seq_ids[1 : self.T + 1]\n",
    "            loss_mask = mask_ids[1:]\n",
    "\n",
    "            assert x.shape[0] == self.T, f\"x shape mismatch {x.shape}\"\n",
    "            assert y.shape[0] == self.T, f\"y shape mismatch {y.shape}\"\n",
    "            assert loss_mask.shape[0] == self.T, (\n",
    "                f\"loss_mask shape mismatch {loss_mask.shape}\"\n",
    "            )\n",
    "\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            batch_mask.append(loss_mask)\n",
    "\n",
    "        batch_x, batch_y, batch_mask = (\n",
    "            torch.stack(batch_x),\n",
    "            torch.stack(batch_y),\n",
    "            torch.stack(batch_mask),\n",
    "        )\n",
    "\n",
    "        assert batch_x.shape == (\n",
    "            self.B,\n",
    "            self.T,\n",
    "        ), f\"batch_x shape mismatch: {batch_x.shape}\"\n",
    "        assert batch_y.shape == (\n",
    "            self.B,\n",
    "            self.T,\n",
    "        ), f\"batch_y shape mismatch: {batch_y.shape}\"\n",
    "        assert batch_mask.shape == (\n",
    "            self.B,\n",
    "            self.T,\n",
    "        ), f\"batch_mask shape mismatch: {batch_mask.shape}\"\n",
    "\n",
    "        # Return batch of shape B,T\n",
    "\n",
    "        return batch_x, batch_y, batch_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Adapter Based on LoRA Method\n",
    "\n",
    "In this section, we will create an adapter using the LoRA (Low-Rank Adaptation) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQN2NKphjCUh"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdapterModel:\n",
    "    \"\"\"\n",
    "    AdapterModel class wraps a base model to prepare it for parameter-efficient fine-tuning.\n",
    "\n",
    "    I created this to inject adapter layers (with A and B matrices) into specific target modules\n",
    "    of the model we want to fine-tune. The class takes:\n",
    "\n",
    "    - `model`: the base pre-trained model (like GPT-2) that we want to fine-tune\n",
    "    - `target_modules`: a list of strings identifying which submodules to modify (like attention layers)\n",
    "\n",
    "    The `_frozen_weight_base_model()` method freezes all the original weights so we only train adapters.\n",
    "\n",
    "    The `_adapt_layer()` method walks through the model structure to find each target module\n",
    "    (either `nn.Linear` or GPT-2 style Conv1D), and replaces it with a `LoraAdapter` layer\n",
    "    that wraps the original one. This is done by recursively traversing the model using the module name path.\n",
    "\n",
    "    The `get_model()` function just returns the adapted model, ready for training only the injected parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, target_modules: list[str]):\n",
    "        self.model = model\n",
    "        self.target_modules = target_modules\n",
    "        self._frozen_weight_base_model(model)\n",
    "        self._adapt_layer()\n",
    "\n",
    "    def _frozen_weight_base_model(self, model):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def _adapt_layer(self):\n",
    "        def is_gpt2_conv1d(module):\n",
    "            return hasattr(module, \"nf\") and hasattr(module, \"nx\")\n",
    "\n",
    "        def is_target_linear(module):\n",
    "            return hasattr(module, \"in_features\") and hasattr(module, \"out_features\")\n",
    "\n",
    "        for target in self.target_modules:\n",
    "            for name, m in self.model.named_modules():\n",
    "                if (is_target_linear(m) or is_gpt2_conv1d(m)) and (target in name):\n",
    "                    path = name.split(\".\")\n",
    "                    current_parent = self.model\n",
    "                    for module_name in path:\n",
    "                        child = getattr(current_parent, module_name)\n",
    "                        if target == module_name:\n",
    "                            setattr(\n",
    "                                current_parent,\n",
    "                                module_name,\n",
    "                                LoraAdapter(base_module=child),\n",
    "                            )\n",
    "                        current_parent = child\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class LoraAdapter(nn.Module):\n",
    "    \"\"\"LoraAdapter wraps a base module (Linear or GPT-2 style Conv1D) and injects two trainable low-rank matrices A and B.\n",
    "\n",
    "    I designed this to simulate the LoRA mechanism by keeping the original weights frozen and only training the adapter layers.\n",
    "\n",
    "    - `base_module`: the original frozen layer (e.g., Linear or Conv1D) we want to wrap\n",
    "    - `rank`: the low-rank dimension used for the adapter (default 8)\n",
    "    - `lora_alpha`: a scaling factor applied to the adapter output\n",
    "    - `lora_dropout`: dropout applied between A and B during training\n",
    "\n",
    "    In the constructor:\n",
    "    - I infer the input and output dimensions from the base module\n",
    "    - I initialize matrix A (input â†’ rank) and B (rank â†’ output)\n",
    "    - I set the weights of B to zeros so that the adapter does nothing at the beginning â€” the model behaves like the base one initially\n",
    "\n",
    "    In the `forward()`:\n",
    "    - I run the input `x` through the base module to get `base_out`\n",
    "    - Then I compute the adapter output via A and B (with dropout), scale it by `lora_alpha`, and add it to `base_out`\n",
    "\n",
    "    This way, the model learns to adjust its behavior through A and B while keeping the original weights intact.\n",
    "    magics ! :))))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_module,\n",
    "        rank: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_module = base_module\n",
    "        self.lora_alpha = lora_alpha\n",
    "        if hasattr(base_module, \"in_features\") and hasattr(base_module, \"out_features\"):\n",
    "            in_dim = base_module.in_features\n",
    "            out_dim = base_module.out_features\n",
    "        elif hasattr(base_module, \"nx\") and hasattr(base_module, \"nf\"):\n",
    "            in_dim = base_module.nx\n",
    "            out_dim = base_module.nf\n",
    "        else:\n",
    "            raise ValueError(\"Module type not supported for LoRA adapter\")\n",
    "\n",
    "        self.adapter_A = nn.Linear(in_dim, rank, bias=False)\n",
    "        self.adapter_B = nn.Linear(rank, out_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=lora_dropout)\n",
    "        torch.nn.init.zeros_(self.adapter_B.weight)\n",
    "\n",
    "        torch.nn.init.normal_(self.adapter_A.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.adapter_A = self.adapter_A.to(dtype=x.dtype)\n",
    "        self.adapter_B = self.adapter_B.to(dtype=x.dtype)\n",
    "        self.dropout = self.dropout.to(dtype=x.dtype)\n",
    "\n",
    "        base_out = self.base_module(x)\n",
    "        lora_out = self.dropout(self.adapter_B(self.adapter_A(x)))\n",
    "\n",
    "        return base_out + self.lora_alpha * lora_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7D5s-JIfV0n",
    "outputId": "fc2d4127-7bc8-4015-eea7-15fae87f29c7"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPqh6YDmywMl",
    "outputId": "1213bdef-c41d-48a8-c971-bbe59a2d9e45"
   },
   "outputs": [],
   "source": [
    "lora_model = AdapterModel(model, [\"c_attn\"])\n",
    "model = lora_model.get_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njqX_JshNgNG"
   },
   "outputs": [],
   "source": [
    "model = lora_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVtPVHcutQui",
    "outputId": "ae882e2a-2f7a-462f-d0d7-52cec5849609"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "model.to(device)\n",
    "torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDX76in63BIF",
    "outputId": "49592afb-08c0-4a66-9229-6e64df610ae0"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} | shape: {param.shape} | requires_grad={param.requires_grad}\")\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "total_trainable = sum(p.numel() for p in trainable_params)\n",
    "print(f\"Nombre total d'Ã©lÃ©ments trainables: {total_trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpE-m_3p7qeX"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], 1e-5)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=600, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lJl6-0wopWL-",
    "outputId": "9da25426-d4f3-4daa-ec17-183d7b4f2a20"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "dataloader = DataLoader(B=6, T=512)\n",
    "\n",
    "\n",
    "def cross_entropy_with_mask(logits, labels, mask):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss using a mask to ignore some tokens.\n",
    "\n",
    "    Given the logits from the model's inference, we use the mask generated by the dataloader\n",
    "    to decide which tokens in the labels we want to compute the loss on. If the mask is 0,\n",
    "    we set the corresponding label to -100 so it gets ignored by PyTorch's cross-entropy loss\n",
    "    (with ignore_index=-100). This way, loss.backward() will only optimize the weights for the\n",
    "    positions where the mask is 1, meaning where we want to compute the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    label_masked = labels.clone()\n",
    "    label_masked[mask == 0] = -100\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.view(B * T, V)\n",
    "    labels_flat = label_masked.view(B * T)\n",
    "    loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=-100)\n",
    "    return loss\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "losses = []\n",
    "steps = []\n",
    "step = 0\n",
    "for epoch in range(25):\n",
    "    batchs = [dataloader.load_batch(dataset=dataset, mode=\"train\") for _ in range(30)]\n",
    "    x, y, mask = batchs[0]\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(batchs):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        x, y, mask = batch\n",
    "        x = x.to(device=device, dtype=torch.long)\n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Autocast enable mixed precision because we load model in float16\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(input_ids=x, labels=y)\n",
    "            loss = cross_entropy_with_mask(out.logits, y, mask)\n",
    "\n",
    "        # we have to scale grad\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        step += 1\n",
    "        steps.append(step)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} Batch {idx} - Loss: {loss.item():.4f} \"\n",
    "            f\"Avg: {total_loss / (idx + 1):.4f}\"\n",
    "        )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(steps, losses)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLrzLj16c_kv"
   },
   "outputs": [],
   "source": [
    "def generate_sample(model, tokenizer, prompt, max_len=20, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        for _ in range(max_len):\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "prompt = \"what is a LLM?\"\n",
    "generated = generate_sample(model, tokenizer, prompt, max_len=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "m9yRWeO7dknH",
    "outputId": "011b58ce-084b-402d-921c-a2c4a790f93f"
   },
   "outputs": [],
   "source": [
    "generated"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
